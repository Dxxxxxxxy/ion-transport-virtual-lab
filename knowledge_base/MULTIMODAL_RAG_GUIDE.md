# Multimodal RAG System - User Guide

## Overview

The Ion Transport Knowledge Base now supports **full multimodal RAG** with three levels of functionality:

### âœ… Level 1: Vision LLM Figure Descriptions
- Extracts all figures/images from PDFs using PyMuPDF
- Analyzes each figure with GPT-4 Vision (GPT-4V)
- Generates detailed descriptions, identifies figure types
- Extracts key insights and approximate values

### âœ… Level 2: Plot Data Extraction
- Detects plot types (XY, bar, heatmap, scatter)
- Extracts numerical data points from graphs
- Stores structured data (axis info, data points, trends)
- Works for XY plots, line graphs, scatter plots

### âœ… Level 3: Multimodal Embeddings and Retrieval
- Generates semantic embeddings for figures
- Combines image analysis with text embeddings
- Enables hybrid search (text + figures)
- Stores images locally with metadata

## Architecture

```
PDF Paper
â”œâ”€â”€ Text Extraction (PyMuPDF)
â”‚   â”œâ”€â”€ Full text content
â”‚   â”œâ”€â”€ Metadata (DOI, citation)
â”‚   â””â”€â”€ Text chunks (1000 tokens)
â”‚
â””â”€â”€ Figure Extraction (Multimodal)
    â”œâ”€â”€ Image files â†’ saved to extracted_figures/{domain}/
    â”œâ”€â”€ GPT-4V Analysis
    â”‚   â”œâ”€â”€ Figure type identification
    â”‚   â”œâ”€â”€ Detailed description
    â”‚   â”œâ”€â”€ Key insights extraction
    â”‚   â””â”€â”€ Approximate values
    â”œâ”€â”€ Plot Data Extraction (for graphs)
    â”‚   â”œâ”€â”€ Axis information
    â”‚   â”œâ”€â”€ Data points (x, y coordinates)
    â”‚   â””â”€â”€ Trends analysis
    â””â”€â”€ Multimodal Embeddings
        â”œâ”€â”€ Text embedding (OpenAI)
        â”œâ”€â”€ Image-aware embedding
        â””â”€â”€ Stored in ChromaDB
```

## Usage

### Basic Usage (Multimodal Enabled by Default)

```bash
# Ingest PDFs with full multimodal processing
python -m ion_transport.knowledge_base.ingest_papers
```

### Advanced Options

```bash
# Disable multimodal processing (text-only mode)
python -m ion_transport.knowledge_base.ingest_papers --no-multimodal

# Show statistics only
python -m ion_transport.knowledge_base.ingest_papers --stats

# Help
python -m ion_transport.knowledge_base.ingest_papers --help
```

## What Gets Extracted

### For Each Figure:

1. **Image File**
   - Saved to: `knowledge_base/extracted_figures/{domain}/{pdf_name}_page{N}_img{M}.{ext}`
   - Minimum size: 100x100 pixels
   - Formats: JPEG, PNG, etc.

2. **Vision Analysis** (GPT-4V)
   ```json
   {
     "figure_type": "XY plot",
     "description": "Capacitance vs pore size showing maximum at 0.7nm",
     "key_insights": ["Peak at 0.7nm", "Decreases for larger pores"],
     "approximate_values": "Peak ~200 F/g at 0.7nm",
     "variables": {
       "x_axis": "Pore size (nm)",
       "y_axis": "Capacitance (F/g)"
     },
     "data_extractable": true
   }
   ```

3. **Plot Data** (if applicable)
   ```json
   {
     "axis_info": {
       "x_axis": {"label": "Pore size", "unit": "nm", "range": [0.5, 3.0]},
       "y_axis": {"label": "Capacitance", "unit": "F/g", "range": [0, 250]}
     },
     "data_points": [
       [0.5, 120], [0.7, 200], [1.0, 180], ...
     ],
     "trends": "Increasing from 0.5-0.7nm, then decreasing"
   }
   ```

4. **ChromaDB Entry**
   - **Text**: Rich description combining caption + analysis + data
   - **Metadata**: PDF source, page number, figure type, image path
   - **Embedding**: Semantic vector for retrieval
   - **Content Type**: Marked as "figure" for filtering

## Querying Multimodal Content

When querying the knowledge base, figures are automatically included:

```python
# Example: Query will return both text chunks AND figure descriptions
query = "What is the optimal pore size for capacitance?"

# Results might include:
# 1. Text chunk: "...pore size of 0.7nm shows maximum capacitance..."
# 2. Figure chunk: "XY plot showing capacitance vs pore size. Peak at 0.7nm with 200 F/g..."
```

### Filtering by Content Type

```python
# Get only figures
results = collection.query(
    query_texts=["pore size capacitance"],
    where={"content_type": "figure"},
    n_results=10
)

# Get only text
results = collection.query(
    query_texts=["pore size capacitance"],
    where={"content_type": "text"},  # Default
    n_results=10
)

# Get specific figure types
results = collection.query(
    query_texts=["concentration profile"],
    where={"figure_type": "XY plot"},
    n_results=5
)
```

## Output Structure

After ingestion, you'll have:

```
ion_transport/
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ extracted_figures/          # NEW: All extracted figures
â”‚   â”‚   â”œâ”€â”€ electrochemistry/
â”‚   â”‚   â”‚   â”œâ”€â”€ Smith_2023_ACS_page3_img0.png
â”‚   â”‚   â”‚   â”œâ”€â”€ Smith_2023_ACS_page5_img1.png
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ membrane_science/
â”‚   â”‚   â”œâ”€â”€ biology/
â”‚   â”‚   â””â”€â”€ nanofluidics/
â”‚   â””â”€â”€ pdfs/
â”‚       â””â”€â”€ [your PDF files]
â””â”€â”€ vector_db/
    â””â”€â”€ chroma.sqlite3              # Contains text + figure embeddings
```

## Cost Estimates

### Multimodal Processing Costs (per PDF):

Assuming average scientific paper with:
- **Text**: 20-30 pages â†’ ~$0.01-0.02 (text processing + embeddings)
- **Figures**: 5-10 figures â†’ ~$0.05-0.15 (GPT-4V analysis)
  - Vision analysis: ~$0.01 per figure (GPT-4V input tokens)
  - Plot data extraction: ~$0.01 per plot (if applicable)
  - Embeddings: ~$0.0001 per figure (text-embedding-3-small)

**Total per PDF**: ~$0.06-0.17

**For 200 PDFs**: ~$12-34

Compare to text-only mode: ~$2-4 for 200 PDFs

## Performance

- **Text-only ingestion**: ~2-3 seconds/PDF
- **Multimodal ingestion**: ~10-15 seconds/PDF (due to GPT-4V calls)
  - Image extraction: ~1 sec
  - Vision analysis: ~5-10 sec (GPT-4V API calls)
  - Plot data extraction: ~3-5 sec (if applicable)
  - Embeddings: ~1 sec

## Limitations

1. **Plot Data Extraction Accuracy**
   - Vision LLM provides approximate values, not pixel-perfect
   - Best for clean, simple plots
   - Complex/overlapping plots: less accurate
   - Use original data files for high-precision needs

2. **Figure Recognition**
   - Minimum size: 100x100 pixels
   - Some small embedded graphics may be missed
   - Page decorations/logos may be extracted (usually filtered by size)

3. **Caption Matching**
   - Captions matched by page proximity (may not be perfect)
   - Some figures may not have captions extracted

## Best Practices

1. **First Run**: Start with a small test (5-10 PDFs) to verify everything works
2. **Cost Management**: Monitor OpenAI API usage during large ingestions
3. **Incremental Updates**: Use smart incremental ingestion to avoid reprocessing
4. **Quality Check**: Spot-check extracted figures in `extracted_figures/` folder
5. **Backup**: Keep original PDFs; extracted data can be regenerated

## Troubleshooting

### Issue: No figures extracted
**Solution**: Check if PDFs have actual embedded images (not scanned bitmap pages)

### Issue: GPT-4V API errors
**Solution**: Check OpenAI API key, rate limits, and account status

### Issue: Plot data extraction fails
**Solution**: This is normal for non-plot figures; extraction is attempted only for graphs

### Issue: High costs
**Solution**: Use `--no-multimodal` flag to disable figure processing temporarily

## Examples

### Example 1: Full Multimodal Ingestion
```bash
cd "/Users/xiaoyangdu/Library/Mobile Documents/com~apple~CloudDocs/HKUST/AI for system ionics/Virtual Lab for system ionics"
source ~/.zshrc
python -m ion_transport.knowledge_base.ingest_papers
```

Output:
```
ğŸš€ ION TRANSPORT KNOWLEDGE BASE INGESTION
================================
Multimodal RAG: âœ“ ENABLED (figures will be extracted & analyzed)

Processing domain: electrochemistry
  ğŸ–¼ï¸  Extracting multimodal content from: Smith_2023_ACS.pdf
    âœ“ Extracted 8 images
    ğŸ” Analyzing Smith_2023_ACS_page3_img0.png with GPT-4V...
    ğŸ“Š Extracting plot data...
    âœ“ Created 8 searchable figure chunks
  âœ“ Extracted 142 chunks from 25 pages
  âœ“ Ingested 142 text chunks
  âœ“ Ingested 8 figure chunks with multimodal analysis
```

### Example 2: Text-Only Mode
```bash
python -m ion_transport.knowledge_base.ingest_papers --no-multimodal
```

Output:
```
Multimodal RAG: âœ— Disabled (text-only mode)
```

## Future Enhancements

Possible improvements:
- [ ] True CLIP embeddings for better image-text alignment
- [ ] OCR for text within figures
- [ ] Chemical structure recognition
- [ ] Table extraction and structuring
- [ ] Equation recognition (LaTeX conversion)
- [ ] Video/animation support

## Support

For issues or questions:
1. Check this guide
2. Review error messages in console output
3. Verify OpenAI API key is set correctly
4. Check `extracted_figures/` folder for output

---

**Status**: âœ… Fully Implemented (Option C Complete)
**Version**: 1.0
**Last Updated**: 2026-01-05
